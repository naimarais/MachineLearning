Table of Contents
Instructions
About the Data
Importing Data
Data Preprocessing
One Hot Encoding
Train and Test Data Split
Train Logistic Regression, KNN, Decision Tree, SVM, and Linear Regression models and return their appropriate accuracy scores
Estimated Time Needed: 180 min

Instructions
In this notebook, you will practice all the classification algorithms that we have learned in this course.

Below, is where we are going to use the classification algorithms to create a model based on our training data and evaluate our testing data using evaluation metrics learned in the course.

We will use some of the algorithms taught in the course, specifically:

Linear Regression
KNN
Decision Trees
Logistic Regression
SVM
We will evaluate our models using:

Accuracy Score
Jaccard Index
F1-Score
LogLoss
Mean Absolute Error
Mean Squared Error
R2-Score
Finally, you will use your models to generate the report at the end.

About The Dataset
The original source of the data is Australian Government's Bureau of Meteorology and the latest data can be gathered from http://www.bom.gov.au/climate/dwo/.

The dataset to be used has extra columns like 'RainToday' and our target is 'RainTomorrow', which was gathered from the Rattle at https://bitbucket.org/kayontoga/rattle/src/master/data/weatherAUS.RData

This dataset contains observations of weather metrics for each day from 2008 to 2017. The weatherAUS.csv dataset includes the following fields:

Field	Description	Unit	Type
Date	Date of the Observation in YYYY-MM-DD	Date	object
Location	Location of the Observation	Location	object
MinTemp	Minimum temperature	Celsius	float
MaxTemp	Maximum temperature	Celsius	float
Rainfall	Amount of rainfall	Millimeters	float
Evaporation	Amount of evaporation	Millimeters	float
Sunshine	Amount of bright sunshine	hours	float
WindGustDir	Direction of the strongest gust	Compass Points	object
WindGustSpeed	Speed of the strongest gust	Kilometers/Hour	object
WindDir9am	Wind direction averaged of 10 minutes prior to 9am	Compass Points	object
WindDir3pm	Wind direction averaged of 10 minutes prior to 3pm	Compass Points	object
WindSpeed9am	Wind speed averaged of 10 minutes prior to 9am	Kilometers/Hour	float
WindSpeed3pm	Wind speed averaged of 10 minutes prior to 3pm	Kilometers/Hour	float
Humidity9am	Humidity at 9am	Percent	float
Humidity3pm	Humidity at 3pm	Percent	float
Pressure9am	Atmospheric pressure reduced to mean sea level at 9am	Hectopascal	float
Pressure3pm	Atmospheric pressure reduced to mean sea level at 3pm	Hectopascal	float
Cloud9am	Fraction of the sky obscured by cloud at 9am	Eights	float
Cloud3pm	Fraction of the sky obscured by cloud at 3pm	Eights	float
Temp9am	Temperature at 9am	Celsius	float
Temp3pm	Temperature at 3pm	Celsius	float
RainToday	If there was rain today	Yes/No	object
RainTomorrow	If there is rain tomorrow	Yes/No	float
Column definitions were gathered from http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml

Import the required libraries
# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.
# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1
# Note: If your environment doesn't support "!mamba install", use "!pip install"
# Surpress warnings:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
#you are running the lab in your  browser, so we will install the libraries using ``piplite``
import piplite
await piplite.install(['pandas'])
await piplite.install(['numpy'])
​
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn import preprocessing
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from sklearn.metrics import jaccard_score
from sklearn.metrics import f1_score
from sklearn.metrics import log_loss
from sklearn.metrics import confusion_matrix, accuracy_score
import sklearn.metrics as metrics
Importing the Dataset
from pyodide.http import pyfetch
​
async def download(url, filename):
    response = await pyfetch(url)
    if response.status == 200:
        with open(filename, "wb") as f:
            f.write(await response.bytes())
path='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillUp/labs/ML-FinalAssignment/Weather_Data.csv'
await download(path, "Weather_Data.csv")
filename ="Weather_Data.csv"
df = pd.read_csv("Weather_Data.csv")
df.head()
Date	MinTemp	MaxTemp	Rainfall	Evaporation	Sunshine	WindGustDir	WindGustSpeed	WindDir9am	WindDir3pm	...	Humidity9am	Humidity3pm	Pressure9am	Pressure3pm	Cloud9am	Cloud3pm	Temp9am	Temp3pm	RainToday	RainTomorrow
0	2/1/2008	19.5	22.4	15.6	6.2	0.0	W	41	S	SSW	...	92	84	1017.6	1017.4	8	8	20.7	20.9	Yes	Yes
1	2/2/2008	19.5	25.6	6.0	3.4	2.7	W	41	W	E	...	83	73	1017.9	1016.4	7	7	22.4	24.8	Yes	Yes
2	2/3/2008	21.6	24.5	6.6	2.4	0.1	W	41	ESE	ESE	...	88	86	1016.7	1015.6	7	8	23.5	23.0	Yes	Yes
3	2/4/2008	20.2	22.8	18.8	2.2	0.0	W	41	NNE	E	...	83	90	1014.2	1011.8	8	8	21.4	20.9	Yes	Yes
4	2/5/2008	19.7	25.7	77.4	4.8	0.0	W	41	NNE	W	...	88	74	1008.3	1004.8	8	8	22.5	25.5	Yes	Yes
5 rows × 22 columns

Data Preprocessing
One Hot Encoding
First, we need to perform one hot encoding to convert categorical variables to binary variables.

df_sydney_processed = pd.get_dummies(data=df, columns=['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])
Next, we replace the values of the 'RainTomorrow' column changing them from a categorical column to a binary column. We do not use the get_dummies method because we would end up with two columns for 'RainTomorrow' and we do not want, since 'RainTomorrow' is our target.

df_sydney_processed.replace(['No', 'Yes'], [0,1], inplace=True)
Training Data and Test Data
Now, we set our 'features' or x values and our Y or target variable.

df_sydney_processed.drop('Date',axis=1,inplace=True)
df_sydney_processed = df_sydney_processed.astype(float)
features = df_sydney_processed.drop(columns='RainTomorrow', axis=1)
Y = df_sydney_processed['RainTomorrow']

Linear Regression
#Enter Your Code, Execute and take the Screenshot

x_train, x_test, y_train, y_test = train_test_split(features, Y, test_size=0.2, random_state=10)
print ('Train set:', x_train.shape, y_train.shape)
print ('Test set:', x_test.shape, y_test.shape)
Train set: (2616, 66) (2616,)
Test set: (655, 66) (655,)

Q2) Create and train a Linear Regression model called LinearReg using the training data (x_train, y_train).
#Enter Your Code, Execute and take the Screenshot
from sklearn import linear_model
LinearReg = linear_model.LinearRegression()
LinearReg.fit(x_train, y_train)

LinearRegression
LinearRegression()

Q3) Now use the predict method on the testing data (x_test) and save it to the array predictions.
#Enter Your Code, Execute and take the Screenshot
predictions = LinearReg.predict(x_test)

Q4) Using the predictions and the y_test dataframe calculate the value for each metric using the appropriate function.
#Enter Your Code, Execute and take the Screenshot
LinearRegression_R2
from sklearn.metrics import r2_score
LinearRegression_MAE = np.mean(np.absolute(predictions - y_test))
LinearRegression_MSE = np.mean((predictions - y_test)**2)
LinearRegression_R2 =  r2_score(y_test , predictions)
​
print( 'MAE:', LinearRegression_MAE)
print('MSE:', LinearRegression_MSE)
print ('R2:',LinearRegression_R2)
MAE: 0.2563193284828244
MSE: 0.1157212427649327
R2: 0.4271288403809509

Q5) Show the MAE, MSE, and R2 in a tabular format using data frame for the linear model.
#Enter Your Code, Execute and take the Screenshot
Report = {"Metrics" :["MAE", "MSE", "R2"], "Values" : [LinearRegression_MAE, LinearRegression_MSE, LinearRegression_R2]}
Linear_Report= pd.DataFrame(Report)
Linear_Report
Report = {"Metrics" :["MAE", "MSE", "R2"], "Values" : [LinearRegression_MAE, LinearRegression_MSE, LinearRegression_R2]}
Linear_Report= pd.DataFrame(Report)
Linear_Report
          
Metrics	Values
0	MAE	0.256319
1	MSE	0.115721
2	R2	0.427129
KNN

Q6) Create and train a KNN model called KNN using the training data (x_train, y_train) with the n_neighbors parameter set to 4.
#Enter Your Code Below, Execute, and Save the Screenshot of the Final Output
KNN = KNeighborsClassifier(n_neighbors = 4).fit(x_train,y_train)

Q7) Now use the predict method on the testing data (x_test) and save it to the array predictions.
#Enter Your Code Below, Execute, and Save the Screenshot of the Final Output
predictions = KNN.predict(x_test)

Q8) Using the predictions and the y_test dataframe calculate the value for each metric using the appropriate function.
#Enter Your Code Below, Execute, and Save the Screenshot of the Final Output
KNN_Accuracy_Score = metrics.accuracy_score(y_test, predictions)
KNN_JaccardIndex = jaccard_score(y_test, predictions)
KNN_F1_Score = f1_score(y_test, predictions)

KNN_Accuracy_Score = metrics.accuracy_score(y_test, predictions)
KNN_JaccardIndex = jaccard_score(y_test, predictions)
KNN_F1_Score = f1_score(y_test, predictions)
​
print("KNN_Accuracy_Score:", KNN_Accuracy_Score )
print("KNN_JaccardIndex:", KNN_JaccardIndex)
print("KNN_F1_Score:", KNN_F1_Score)
KNN_Accuracy_Score: 0.8183206106870229
KNN_JaccardIndex: 0.4251207729468599
KNN_F1_Score: 0.5966101694915255
Decision Tree

Q9) Create and train a Decision Tree model called Tree using the training data (x_train, y_train).
#Enter Your Code, Execute and take the Screenshot
Tree = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
Tree.fit(x_train, y_train)

DecisionTreeClassifier
DecisionTreeClassifier(criterion='entropy', max_depth=4)

Q10) Now use the predict method on the testing data (x_test) and save it to the array predictions.
#Enter Your Code, Execute and take the Screenshot
predictions = Tree.predict(x_test)

Q11) Using the predictions and the y_test dataframe calculate the value for each metric using the appropriate function.
#Enter Your Code, Execute and take the Screenshot
Tree_Accuracy_Score = metrics.accuracy_score(y_test, predictions)
Tree_JaccardIndex = jaccard_score(y_test, predictions)
Tree_F1_Score = f1_score(y_test, predictions)
​
print("Tree_Accuracy_Score:", Tree_Accuracy_Score)
print("Tree_JaccardIndex:", Tree_JaccardIndex)
print("Tree_F1_Score:", Tree_F1_Score)
Tree_Accuracy_Score: 0.8183206106870229
Tree_JaccardIndex: 0.48034934497816595
Tree_F1_Score: 0.6489675516224188
Logistic Regression

Q12) Use the train_test_split function to split the features and Y dataframes with a test_size of 0.2 and the random_state set to 1.
#Enter Your Code, Execute and take the Screenshot
Y
x_train, x_test, y_train, y_test = train_test_split( features, Y, test_size=0.2, random_state=1)

Q13) Create and train a LogisticRegression model called LR using the training data (x_train, y_train) with the solver parameter set to liblinear.
#Enter Your Code, Execute and take the Screenshot
LR
LR = LogisticRegression(C=0.01, solver='liblinear').fit(x_train,y_train)
LR

LogisticRegression
LogisticRegression(C=0.01, solver='liblinear')

Q14) Now, use the predict and predict_proba methods on the testing data (x_test) and save it as 2 arrays predictions and predict_proba.
#Enter Your Code, Execute and take the Screenshot
predictions = LR.predict(x_test)
​
predict_proba
predict_proba = LR.predict_proba(x_test)
predict_proba
array([[0.71865751, 0.28134249],
       [0.9758825 , 0.0241175 ],
       [0.49701165, 0.50298835],
       ...,
       [0.97264147, 0.02735853],
       [0.73340911, 0.26659089],
       [0.38535984, 0.61464016]])

Q15) Using the predictions, predict_proba and the y_test dataframe calculate the value for each metric using the appropriate function.
#Enter Your Code, Execute and take the Screenshot
LR_F1_Score
LR_Accuracy_Score = metrics.accuracy_score(y_test, predictions)
LR_JaccardIndex = jaccard_score(y_test, predictions)
LR_F1_Score = f1_score(y_test, predictions)
LR_Log_Loss = log_loss(y_test, predict_proba)
​
print("LR_Accuracy_Score:", LR_Accuracy_Score)
print("LR_JaccardIndex:", LR_JaccardIndex)
print("LR_F1_Score:", LR_F1_Score)
print("LR_Log_Loss:", LR_Log_Loss)
LR_Accuracy_Score: 0.8274809160305343
LR_JaccardIndex: 0.4840182648401826
LR_F1_Score: 0.6523076923076923
LR_Log_Loss: 0.3800847372796161

SVM
Q16) Create and train a SVM model called SVM using the training data (x_train, y_train).
#Enter Your Code Below, Execute, and Save the Screenshot of the Final Output
SVM = svm.SVC(kernel= "rbf")
SVM.fit(x_train, y_train)

SVC
SVC()

Q17) Now use the predict method on the testing data (x_test) and save it to the array predictions.
#Enter Your Code Below, Execute, and Save the Screenshot of the Final Output
predictions = SVM.predict(x_test)
​
Q18) Using the predictions and the y_test dataframe calculate the value for each metric using the appropriate function.
SVM_Accuracy_Score = metrics.accuracy_score(y_test, predictions)
SVM_JaccardIndex = jaccard_score(y_test, predictions)
SVM_F1_Score = f1_score(y_test, predictions)
​
print("SVM_Accuracy_Score:", SVM_Accuracy_Score)
print("SVM_JaccardIndex:", SVM_JaccardIndex)
print("SVM_F1_Score:", SVM_F1_Score)
SVM_Accuracy_Score: 0.7221374045801526
SVM_JaccardIndex: 0.0
SVM_F1_Score: 0.0
Report

Q19) Show the Accuracy,Jaccard Index,F1-Score and LogLoss in a tabular format using data frame for all of the above models.
*LogLoss is only for Logistic Regression Model

List_acc=[KNN_Accuracy_Score,Tree_Accuracy_Score,LR_Accuracy_Score,SVM_Accuracy_Score]
List_jac=[KNN_JaccardIndex,Tree_JaccardIndex,LR_JaccardIndex, SVM_JaccardIndex]
List_F1= [KNN_F1_Score,Tree_F1_Score,LR_F1_Score,SVM_F1_Score]
List_Log=['NA','NA', LR_Log_Loss,'NA']
​
​
Report = pd.DataFrame(List_acc, index=['KNN', 'Decision Tree', 'Logistic regression', 'SVM'])
Report.columns=['Accuracy_score']
Report.insert(loc=1, column='Jaccard_Index', value= List_jac)
Report.insert(loc=2, column='F1_score', value= List_F1)
Report.insert(loc=3, column='Log_loss', value= List_Log)
​
​
Report   
Accuracy_score	Jaccard_Index	F1_score	Log_loss
KNN	0.818321	0.425121	0.596610	NA
Decision Tree	0.818321	0.480349	0.648968	NA
Logistic regression	0.827481	0.484018	0.652308	0.380085
SVM	0.722137	0.000000	0.000000	NA
